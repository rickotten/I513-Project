{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accepting-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic analysis to try and fix our mistakes from last time\n",
    "# We do:\n",
    "# * Pre-processing of data\n",
    "# * Feature Selection\n",
    "# * Model Evaluation\n",
    "# * Scoring Metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-omaha",
   "metadata": {},
   "source": [
    "### Data Collection and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "grave-occurrence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Marketcap</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-04-29</th>\n",
       "      <td>147.488007</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>134.444000</td>\n",
       "      <td>144.539993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.603769e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-30</th>\n",
       "      <td>146.929993</td>\n",
       "      <td>134.050003</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.542813e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-01</th>\n",
       "      <td>139.889999</td>\n",
       "      <td>107.720001</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>116.989998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.298955e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-02</th>\n",
       "      <td>125.599998</td>\n",
       "      <td>92.281898</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>105.209999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.168517e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-03</th>\n",
       "      <td>108.127998</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>106.250000</td>\n",
       "      <td>97.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.085995e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close  Volume  \\\n",
       "Date                                                                 \n",
       "2013-04-29  147.488007  134.000000  134.444000  144.539993     0.0   \n",
       "2013-04-30  146.929993  134.050003  144.000000  139.000000     0.0   \n",
       "2013-05-01  139.889999  107.720001  139.000000  116.989998     0.0   \n",
       "2013-05-02  125.599998   92.281898  116.379997  105.209999     0.0   \n",
       "2013-05-03  108.127998   79.099998  106.250000   97.750000     0.0   \n",
       "\n",
       "               Marketcap  \n",
       "Date                      \n",
       "2013-04-29  1.603769e+09  \n",
       "2013-04-30  1.542813e+09  \n",
       "2013-05-01  1.298955e+09  \n",
       "2013-05-02  1.168517e+09  \n",
       "2013-05-03  1.085995e+09  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Focus on Bitcoin Here\n",
    "\n",
    "coin_df = pd.read_csv('Data/coin_Bitcoin.csv')\n",
    "\n",
    "coin_df.index = pd.to_datetime(coin_df['Date']).dt.date\n",
    "coin_df.index = pd.DatetimeIndex(coin_df.index)\n",
    "\n",
    "# Not useful\n",
    "coin_df.drop('Date', axis=1, inplace=True)\n",
    "coin_df.drop('SNo', axis=1, inplace=True)\n",
    "coin_df.drop('Symbol', axis=1, inplace=True)\n",
    "coin_df.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "coin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rational-asthma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Marketcap</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-04-30</th>\n",
       "      <td>146.929993</td>\n",
       "      <td>134.050003</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.542813e+09</td>\n",
       "      <td>-0.038328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-01</th>\n",
       "      <td>139.889999</td>\n",
       "      <td>107.720001</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>116.989998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.298955e+09</td>\n",
       "      <td>-0.158345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-02</th>\n",
       "      <td>125.599998</td>\n",
       "      <td>92.281898</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>105.209999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.168517e+09</td>\n",
       "      <td>-0.100692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-03</th>\n",
       "      <td>108.127998</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>106.250000</td>\n",
       "      <td>97.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.085995e+09</td>\n",
       "      <td>-0.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-04</th>\n",
       "      <td>115.000000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>98.099998</td>\n",
       "      <td>112.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.250317e+09</td>\n",
       "      <td>0.150895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close  Volume  \\\n",
       "Date                                                                 \n",
       "2013-04-30  146.929993  134.050003  144.000000  139.000000     0.0   \n",
       "2013-05-01  139.889999  107.720001  139.000000  116.989998     0.0   \n",
       "2013-05-02  125.599998   92.281898  116.379997  105.209999     0.0   \n",
       "2013-05-03  108.127998   79.099998  106.250000   97.750000     0.0   \n",
       "2013-05-04  115.000000   92.500000   98.099998  112.500000     0.0   \n",
       "\n",
       "               Marketcap    Return  \n",
       "Date                                \n",
       "2013-04-30  1.542813e+09 -0.038328  \n",
       "2013-05-01  1.298955e+09 -0.158345  \n",
       "2013-05-02  1.168517e+09 -0.100692  \n",
       "2013-05-03  1.085995e+09 -0.070906  \n",
       "2013-05-04  1.250317e+09  0.150895  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a classification column. The class of each data point is determined by the percent change in close price\n",
    "# day by day\n",
    "coin_df['Return'] = coin_df['Close'].pct_change()\n",
    "# First row in df has NaN 'return' since there is no previous day\n",
    "coin_df.drop(coin_df.index[0], inplace=True)\n",
    "coin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "logical-tamil",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Marketcap</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-04-30</th>\n",
       "      <td>146.929993</td>\n",
       "      <td>134.050003</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.542813e+09</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-01</th>\n",
       "      <td>139.889999</td>\n",
       "      <td>107.720001</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>116.989998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.298955e+09</td>\n",
       "      <td>very bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-02</th>\n",
       "      <td>125.599998</td>\n",
       "      <td>92.281898</td>\n",
       "      <td>116.379997</td>\n",
       "      <td>105.209999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.168517e+09</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-03</th>\n",
       "      <td>108.127998</td>\n",
       "      <td>79.099998</td>\n",
       "      <td>106.250000</td>\n",
       "      <td>97.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.085995e+09</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-04</th>\n",
       "      <td>115.000000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>98.099998</td>\n",
       "      <td>112.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.250317e+09</td>\n",
       "      <td>very good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close  Volume  \\\n",
       "Date                                                                 \n",
       "2013-04-30  146.929993  134.050003  144.000000  139.000000     0.0   \n",
       "2013-05-01  139.889999  107.720001  139.000000  116.989998     0.0   \n",
       "2013-05-02  125.599998   92.281898  116.379997  105.209999     0.0   \n",
       "2013-05-03  108.127998   79.099998  106.250000   97.750000     0.0   \n",
       "2013-05-04  115.000000   92.500000   98.099998  112.500000     0.0   \n",
       "\n",
       "               Marketcap     Return  \n",
       "Date                                 \n",
       "2013-04-30  1.542813e+09        bad  \n",
       "2013-05-01  1.298955e+09   very bad  \n",
       "2013-05-02  1.168517e+09        bad  \n",
       "2013-05-03  1.085995e+09        bad  \n",
       "2013-05-04  1.250317e+09  very good  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment on return: \n",
    "# > .2 then very good 0 < < 0.2 then good converse for bad and very bad\n",
    "def get_sentiment(return_val):\n",
    "    if return_val >= 0.15:\n",
    "        return 'very good'\n",
    "    elif return_val > 0 and return_val < 0.15:\n",
    "        return 'good'\n",
    "    elif return_val <= 0 and return_val > -0.15:\n",
    "        return 'bad'\n",
    "    elif return_val <= -0.15:\n",
    "        return 'very bad'\n",
    "    else:\n",
    "        print(return_val)\n",
    "        return None\n",
    "# Replace Return column with label\n",
    "coin_df['Return'] = [get_sentiment(return_val) for return_val in coin_df['Return']]\n",
    "coin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sweet-piece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bad': 1291, 'very bad': 19, 'very good': 18, 'good': 1533}\n"
     ]
    }
   ],
   "source": [
    "# We are faced with a new problem: Class Imbalance. This will be addressed in the next section\n",
    "class_counts = {}\n",
    "for class_ in coin_df['Return']:\n",
    "    class_counts[class_] = class_counts.get(class_, 0) + 1\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-detail",
   "metadata": {},
   "source": [
    "### Model Evaluation/Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "appropriate-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To decide on the best model, we should fine tune the parameters of the models in consideration to see\n",
    "# the best performances of our models.\n",
    "\n",
    "# Because Bitcoin prices can be very volatile with plenty of outliers,\n",
    "# we use RobustScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = coin_df[['Open', 'High', 'Low', 'Volume', 'Marketcap', 'Close']]\n",
    "y = coin_df['Return']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "X_train = transformer.transform(X_train)\n",
    "X_test = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assumed-parts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bad': 0.5561111111111111, 'good': 0.4642857142857143, 'very bad': 41.708333333333336, 'very good': 41.708333333333336}\n"
     ]
    }
   ],
   "source": [
    "# We should now deal with the issue of class_weights\n",
    "from sklearn.utils import class_weight\n",
    "labels = np.unique(y_train)\n",
    "class_weights = dict(zip(labels, class_weight.compute_class_weight('balanced', classes=labels, y=y_train)))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bright-japan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid score: 0.88\n",
      "Grid test score: 0.86\n",
      "Best grid parameters: {'C': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Parameter Tuning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "param_grid = {  'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "                }\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000, class_weight=class_weights)\n",
    "grid = GridSearchCV(lr, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "# lr_pred = lr.predict(X_test)\n",
    "# print('Logistic Reg Score: {:.2f}'.format(lr.score(X_test, y_test)))\n",
    "print(\"Best grid score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Grid test score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best grid parameters: {}\".format(grid.best_params_))\n",
    "lr_best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alpine-north",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid score: 0.71\n",
      "Grid test score: 0.73\n",
      "Best grid parameters: {'criterion': 'gini', 'max_depth': 50}\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Parameter Tuning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "n_components = list(range(1,X.shape[1]+1,1))\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12]\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [2,4,6,8,10,12,14,16,18,20,25,30,40,50,70]\n",
    "}\n",
    "dt = DecisionTreeClassifier(class_weight=class_weights)\n",
    "grid = GridSearchCV(dt, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best grid score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Grid test score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best grid parameters: {}\".format(grid.best_params_))\n",
    "dt_best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lonely-royal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid score: 0.64\n",
      "Grid test score: 0.68\n",
      "Best grid parameters: {'metric': 'euclidean', 'n_neighbors': 3, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors Parameter Tuning\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 11, 19],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "knn = KNeighborsClassifier() # No class_weight param\n",
    "grid = GridSearchCV(knn, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best grid score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Grid test score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best grid parameters: {}\".format(grid.best_params_))\n",
    "knn_best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "polish-habitat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best grid score: 0.87\n",
      "Grid test score: 0.89\n",
      "Best grid parameters: {'C': 1000, 'gamma': 1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# SVC Parameter Tuning\n",
    "from sklearn.svm import SVC\n",
    "param_grid = {\n",
    "    'C':[1,10,100,1000],\n",
    "    'gamma':[1,0.1,0.001,0.0001],\n",
    "    'kernel':['linear','rbf']\n",
    "}\n",
    "svc = SVC(class_weight=class_weights)\n",
    "grid = GridSearchCV(svc, param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best grid score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Grid test score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best grid parameters: {}\".format(grid.best_params_))\n",
    "svc_best_params = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "burning-printer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Score: 0.86\n",
      "Decision Tree Score: 0.73\n",
      "KNN Score: 0.68\n",
      "SVC Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Let's retrain the models using the fine-tuned parameters found\n",
    "# so we can examine the confusion matrix and scoring report and make a conclusion.\n",
    "lr = LogisticRegression(max_iter=2000, C=lr_best_params['C'], class_weight=class_weights)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "print('Logistic Regression Score: {:.2f}'.format(lr.score(X_test, y_test)))\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion=dt_best_params['criterion'], max_depth=dt_best_params['max_depth'], class_weight=class_weights)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "print('Decision Tree Score: {:.2f}'.format(dt.score(X_test, y_test)))\n",
    "\n",
    "knn = KNeighborsClassifier(metric=knn_best_params['metric'], n_neighbors=knn_best_params['n_neighbors'], weights=knn_best_params['weights'])\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "print('KNN Score: {:.2f}'.format(knn.score(X_test, y_test)))\n",
    "\n",
    "svc = SVC(C=svc_best_params['C'], gamma=svc_best_params['gamma'], kernel=svc_best_params['kernel'], class_weight=class_weights)\n",
    "svc.fit(X_train, y_train)\n",
    "svc_pred = svc.predict(X_test)\n",
    "print('SVC Score: {:.2f}'.format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "handmade-korean",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Confusion Matrix:\n",
      "           Bad  Good  Very Good  Very Bad\n",
      "Bad        357     3         31         0\n",
      "Good        55   365         30         5\n",
      "Very Good    0     0          7         0\n",
      "Very Bad     0     0          0         6 \n",
      "\n",
      "Decision Tree Classifier Confusion Matrix:\n",
      "           Bad  Good  Very Good  Very Bad\n",
      "Bad        276   114          1         0\n",
      "Good       106   346          1         2\n",
      "Very Good    4     2          1         0\n",
      "Very Bad     0     5          0         1 \n",
      "\n",
      "KNN Classifier Confusion Matrix:\n",
      "           Bad  Good  Very Good  Very Bad\n",
      "Bad        243   148          0         0\n",
      "Good       115   340          0         0\n",
      "Very Good    5     1          1         0\n",
      "Very Bad     0     5          0         1 \n",
      "\n",
      "SVC Classifier Confusion Matrix:\n",
      "           Bad  Good  Very Good  Very Bad\n",
      "Bad        362     5         24         0\n",
      "Good        44   389         20         2\n",
      "Very Good    0     0          7         0\n",
      "Very Bad     0     2          1         3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "labels = [\"Bad\", \"Good\", \"Very Good\", \"Very Bad\"]\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, lr_pred), index=labels, columns=labels), '\\n')\n",
    "print(\"Decision Tree Classifier Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, dt_pred), index=labels, columns=labels), '\\n')\n",
    "print(\"KNN Classifier Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, knn_pred), index=labels, columns=labels), '\\n')\n",
    "print(\"SVC Classifier Confusion Matrix:\")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, svc_pred), index=labels, columns=labels), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "photographic-affiliate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.87      0.91      0.89       391\n",
      "        good       0.99      0.80      0.89       455\n",
      "    very bad       0.10      1.00      0.19         7\n",
      "   very good       0.55      1.00      0.71         6\n",
      "\n",
      "    accuracy                           0.86       859\n",
      "   macro avg       0.63      0.93      0.67       859\n",
      "weighted avg       0.92      0.86      0.88       859\n",
      " \n",
      "\n",
      "Decision Tree Classifier Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.72      0.71      0.71       391\n",
      "        good       0.74      0.76      0.75       455\n",
      "    very bad       0.33      0.14      0.20         7\n",
      "   very good       0.33      0.17      0.22         6\n",
      "\n",
      "    accuracy                           0.73       859\n",
      "   macro avg       0.53      0.44      0.47       859\n",
      "weighted avg       0.72      0.73      0.72       859\n",
      " \n",
      "\n",
      "KNN Classifier Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.67      0.62      0.64       391\n",
      "        good       0.69      0.75      0.72       455\n",
      "    very bad       1.00      0.14      0.25         7\n",
      "   very good       1.00      0.17      0.29         6\n",
      "\n",
      "    accuracy                           0.68       859\n",
      "   macro avg       0.84      0.42      0.47       859\n",
      "weighted avg       0.68      0.68      0.68       859\n",
      " \n",
      "\n",
      "SVC Classifier Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.89      0.93      0.91       391\n",
      "        good       0.98      0.85      0.91       455\n",
      "    very bad       0.13      1.00      0.24         7\n",
      "   very good       0.60      0.50      0.55         6\n",
      "\n",
      "    accuracy                           0.89       859\n",
      "   macro avg       0.65      0.82      0.65       859\n",
      "weighted avg       0.93      0.89      0.90       859\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, lr_pred), '\\n')\n",
    "print(\"Decision Tree Classifier Classification Report:\")\n",
    "print(classification_report(y_test, dt_pred), '\\n')\n",
    "print(\"KNN Classifier Classification Report:\")\n",
    "print(classification_report(y_test, knn_pred), '\\n')\n",
    "print(\"SVC Classifier Classification Report:\")\n",
    "print(classification_report(y_test, svc_pred), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-dating",
   "metadata": {},
   "source": [
    "The confusion matrices and the classification reports above give us more insight into the performance of the models. From lecture, \"When the skew in the class distributions are severe, accuracy can become an unreliable measure of model performance\". This is apparent here due to the disproportion between \"bad\" and \"good\" data points vs. \"very bad\" and \"very good\" data points.\n",
    "\n",
    "Looking at the metrics for our **LogisticRegression** model, we see from the confusion matrix that its accuracy is not all due to the correct \"good\" and \"bad\" classifications. We see that it was able to classify \"very good\" and \"very bad\" data points accurately. However, when we examing the classification report, we see that the f1-score for \"very bad\" is not ideal. This is due to the fact that while we have the maximum recall, we have a low precision. This is a con of this model that needs to be considered.\n",
    "\n",
    "We can safely eliminate the **DecisionTreeClassifier** and the **KNeighborsClassifier** from our candidate models as they are unable to have any success in predicting the two classes with the fewest data points. This is supported by their respective classification reports.\n",
    "\n",
    "Finally, we have the **SVC Classifier**. This classifier's performance is up-to-par with our **LogisticReegression** classifier (which is the best model seen thus far). We see that while its success with \"very bad\" is not as good as its Logistic counterpart, we observe higher f1-scores. While the f1-scores for \"very bad\" and \"very good\" leave something to be desired, we can say that, overall, the SVC provides us with better metrics.\n",
    "\n",
    "Thus, we will proceed with the SVC Classifier for feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-showcase",
   "metadata": {},
   "source": [
    "### Model-Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "arctic-funeral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with all features: 0.886\n",
      "           Bad  Good  Very Good  Very Bad\n",
      "Bad        362     5         24         0\n",
      "Good        44   389         20         2\n",
      "Very Good    0     0          7         0\n",
      "Very Bad     0     2          1         3 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.89      0.93      0.91       391\n",
      "        good       0.98      0.85      0.91       455\n",
      "    very bad       0.13      1.00      0.24         7\n",
      "   very good       0.60      0.50      0.55         6\n",
      "\n",
      "    accuracy                           0.89       859\n",
      "   macro avg       0.65      0.82      0.65       859\n",
      "weighted avg       0.93      0.89      0.90       859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "labels = [\"Bad\", \"Good\", \"Very Good\", \"Very Bad\"]\n",
    "\n",
    "svc = SVC(C=svc_best_params['C'],\n",
    "          gamma=svc_best_params['gamma'],\n",
    "          kernel=svc_best_params['kernel'],\n",
    "          class_weight=class_weights)\n",
    "\n",
    "select = SelectFromModel(svc, threshold='median')\n",
    "select.fit(X_train, y_train)\n",
    "X_train_fs = select.transform(X_train)\n",
    "\n",
    "# With all features\n",
    "svc.fit(X_train, y_train)\n",
    "print('Score with all features: {:.3f}'.format(svc.score(X_test, y_test)))\n",
    "svc_pred = svc.predict(X_test)\n",
    "print(pd.DataFrame(confusion_matrix(y_test, svc_pred), index=labels, columns=labels), '\\n')\n",
    "print(classification_report(y_test, svc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "reasonable-victory",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['Open', 'High', 'Close']\n",
      "Score with selected features: 0.827\n",
      "           Bad  Good  Very Good  Very Bad\n",
      "Bad        365     2         24         0\n",
      "Good        92   333         30         0\n",
      "Very Good    0     0          7         0\n",
      "Very Bad     0     0          1         5 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bad       0.80      0.93      0.86       391\n",
      "        good       0.99      0.73      0.84       455\n",
      "    very bad       0.11      1.00      0.20         7\n",
      "   very good       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.83       859\n",
      "   macro avg       0.73      0.87      0.70       859\n",
      "weighted avg       0.90      0.83      0.85       859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With selected features\n",
    "svc.fit(X_train_fs, y_train)\n",
    "X_test_fs = select.transform(X_test)\n",
    "svc_pred = svc.predict(X_test_fs)\n",
    "\n",
    "mask = select.get_support()\n",
    "selected_features = []\n",
    "for i in range(len(X.columns)):\n",
    "    if mask[i]:\n",
    "        selected_features += [X.columns[i]]\n",
    "\n",
    "print(f'Selected Features: {selected_features}')\n",
    "print('Score with selected features: {:.3f}'.format(svc.score(X_test_fs, y_test)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, svc_pred), index=labels, columns=labels), '\\n')\n",
    "print(classification_report(y_test, svc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-region",
   "metadata": {},
   "source": [
    "While we observed a drop in accuracy with the selected features, this is not always a bad sign. We want to avoid overfitting, and too many features may contribute to this. Starch improvements in the f1-score for \"very good\" have been noted as well as the noticeable drop in the f1-score of \"very bad\". However, the f1-score improvement for \"very good\" outweighs f1-score decrease for \"very bad\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-jacket",
   "metadata": {},
   "source": [
    "### Cross Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unable-killing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34146341, 0.84965035, 0.64335664, 0.88461538, 0.86013986,\n",
       "       0.97552448, 0.96503497, 0.96853147, 0.98601399, 0.98951049])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have fine-tuned our chose model, we can observe its performance with a cross-fold validation.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# \"Reset\" our data\n",
    "X = coin_df[['Open', 'High', 'Low', 'Volume', 'Marketcap', 'Close']]\n",
    "y = coin_df['Return']\n",
    "\n",
    "# Our best model\n",
    "svc = SVC(C=svc_best_params['C'],\n",
    "          gamma=svc_best_params['gamma'],\n",
    "          kernel=svc_best_params['kernel'],\n",
    "          class_weight=class_weights)\n",
    "\n",
    "# Use a pipeline to include scaling and feature selection\n",
    "clf = make_pipeline(RobustScaler(), SelectFromModel(svc), svc)\n",
    "cross_val_score(clf, X, y, cv=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
